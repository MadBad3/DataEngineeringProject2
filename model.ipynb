{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from unidecode import unidecode\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from nltk.tokenize import word_tokenize\n",
    "import pickle\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "df = pd.read_csv(r'tweets.csv')\n",
    "\n",
    "contractions = {\n",
    "\"ain't\": \"am not / are not\",\n",
    "\"aren't\": \"are not / am not\",\n",
    "\"can't\": \"cannot\",\n",
    "\"can't've\": \"cannot have\",\n",
    "\"'cause\": \"because\",\n",
    "\"could've\": \"could have\",\n",
    "\"couldn't\": \"could not\",\n",
    "\"couldn't've\": \"could not have\",\n",
    "\"didn't\": \"did not\",\n",
    "\"doesn't\": \"does not\",\n",
    "\"don't\": \"do not\",\n",
    "\"hadn't\": \"had not\",\n",
    "\"hadn't've\": \"had not have\",\n",
    "\"hasn't\": \"has not\",\n",
    "\"haven't\": \"have not\",\n",
    "\"he'd\": \"he had / he would\",\n",
    "\"he'd've\": \"he would have\",\n",
    "\"he'll\": \"he shall / he will\",\n",
    "\"he'll've\": \"he shall have / he will have\",\n",
    "\"he's\": \"he has / he is\",\n",
    "\"how'd\": \"how did\",\n",
    "\"how'd'y\": \"how do you\",\n",
    "\"how'll\": \"how will\",\n",
    "\"how's\": \"how has / how is\",\n",
    "\"i'd\": \"I had / I would\",\n",
    "\"i'd've\": \"I would have\",\n",
    "\"i'll\": \"I shall / I will\",\n",
    "\"i'll've\": \"I shall have / I will have\",\n",
    "\"i'm\": \"I am\",\n",
    "\"i've\": \"I have\",\n",
    "\"isn't\": \"is not\",\n",
    "\"it'd\": \"it had / it would\",\n",
    "\"it'd've\": \"it would have\",\n",
    "\"it'll\": \"it shall / it will\",\n",
    "\"it'll've\": \"it shall have / it will have\",\n",
    "\"it's\": \"it has / it is\",\n",
    "\"let's\": \"let us\",\n",
    "\"ma'am\": \"madam\",\n",
    "\"mayn't\": \"may not\",\n",
    "\"might've\": \"might have\",\n",
    "\"mightn't\": \"might not\",\n",
    "\"mightn't've\": \"might not have\",\n",
    "\"must've\": \"must have\",\n",
    "\"mustn't\": \"must not\",\n",
    "\"mustn't've\": \"must not have\",\n",
    "\"needn't\": \"need not\",\n",
    "\"needn't've\": \"need not have\",\n",
    "\"o'clock\": \"of the clock\",\n",
    "\"oughtn't\": \"ought not\",\n",
    "\"oughtn't've\": \"ought not have\",\n",
    "\"shan't\": \"shall not\",\n",
    "\"sha'n't\": \"shall not\",\n",
    "\"shan't've\": \"shall not have\",\n",
    "\"she'd\": \"she had / she would\",\n",
    "\"she'd've\": \"she would have\",\n",
    "\"she'll\": \"she shall / she will\",\n",
    "\"she'll've\": \"she shall have / she will have\",\n",
    "\"she's\": \"she has / she is\",\n",
    "\"should've\": \"should have\",\n",
    "\"shouldn't\": \"should not\",\n",
    "\"shouldn't've\": \"should not have\",\n",
    "\"so've\": \"so have\",\n",
    "\"so's\": \"so as / so is\",\n",
    "\"that'd\": \"that would / that had\",\n",
    "\"that'd've\": \"that would have\",\n",
    "\"that's\": \"that has / that is\",\n",
    "\"there'd\": \"there had / there would\",\n",
    "\"there'd've\": \"there would have\",\n",
    "\"there's\": \"there has / there is\",\n",
    "\"they'd\": \"they had / they would\",\n",
    "\"they'd've\": \"they would have\",\n",
    "\"they'll\": \"they shall / they will\",\n",
    "\"they'll've\": \"they shall have / they will have\",\n",
    "\"they're\": \"they are\",\n",
    "\"they've\": \"they have\",\n",
    "\"to've\": \"to have\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'd\": \"we had / we would\",\n",
    "\"we'd've\": \"we would have\",\n",
    "\"we'll\": \"we will\",\n",
    "\"we'll've\": \"we will have\",\n",
    "\"we're\": \"we are\",\n",
    "\"we've\": \"we have\",\n",
    "\"weren't\": \"were not\",\n",
    "\"what'll\": \"what shall / what will\",\n",
    "\"what'll've\": \"what shall have / what will have\",\n",
    "\"what're\": \"what are\",\n",
    "\"what's\": \"what has / what is\",\n",
    "\"what've\": \"what have\",\n",
    "\"when's\": \"when has / when is\",\n",
    "\"when've\": \"when have\",\n",
    "\"where'd\": \"where did\",\n",
    "\"where's\": \"where has / where is\",\n",
    "\"where've\": \"where have\",\n",
    "\"who'll\": \"who shall / who will\",\n",
    "\"who'll've\": \"who shall have / who will have\",\n",
    "\"who's\": \"who has / who is\",\n",
    "\"who've\": \"who have\",\n",
    "\"why's\": \"why has / why is\",\n",
    "\"why've\": \"why have\",\n",
    "\"will've\": \"will have\",\n",
    "\"won't\": \"will not\",\n",
    "\"won't've\": \"will not have\",\n",
    "\"would've\": \"would have\",\n",
    "\"wouldn't\": \"would not\",\n",
    "\"wouldn't've\": \"would not have\",\n",
    "\"y'all\": \"you all\",\n",
    "\"y'all'd\": \"you all would\",\n",
    "\"y'all'd've\": \"you all would have\",\n",
    "\"y'all're\": \"you all are\",\n",
    "\"y'all've\": \"you all have\",\n",
    "\"you'd\": \"you had / you would\",\n",
    "\"you'd've\": \"you would have\",\n",
    "\"you'll\": \"you shall / you will\",\n",
    "\"you'll've\": \"you shall have / you will have\",\n",
    "\"you're\": \"you are\",\n",
    "\"you've\": \"you have\"\n",
    "}\n",
    "\n",
    "def preprocess2(text):\n",
    "    text2 = []\n",
    "    #lower characters and put it \n",
    "    lowered_text = text.lower()\n",
    "    text2.append(unidecode(lowered_text))\n",
    "        \n",
    "    #Contractions to original length\n",
    "    i = -1\n",
    "    for sent in text2:\n",
    "        i+=1\n",
    "        for word in sent.split():\n",
    "            if word in contractions:\n",
    "                text2[i] = text2[i].replace(word, contractions[word.lower()])\n",
    "\n",
    "    #remove the twitter pictures links\n",
    "    i = -1\n",
    "    for sent in text2:\n",
    "        i+=1\n",
    "        for word in sent.split():\n",
    "            if 'pic.' in word:\n",
    "                #text2[i] = text2[i].replace(word, '')\n",
    "                text2[i] = text2[i].replace(word[word.index(\"pic\"):], '')\n",
    "\n",
    "    #remove all others links that begin with http      \n",
    "    i = -1\n",
    "    for sent in text2:\n",
    "        i+=1\n",
    "        for word in sent.split():\n",
    "            if 'http' in word:\n",
    "                text2[i] = text2[i].replace(word, '')            \n",
    "\n",
    "    #remove all special characters and punctuations\n",
    "    i=0\n",
    "    for sent in text2:\n",
    "        text2[i] = re.sub(r'\\W+', ' ', text2[i])\n",
    "        i+=1    \n",
    "\n",
    "    #remove stopwords\n",
    "    def convert_list_to_string(org_list, seperator=' '):\n",
    "        return seperator.join(org_list)\n",
    "\n",
    "    i = -1\n",
    "    for sent in text2:\n",
    "        i+=1\n",
    "        filtered_sentence = [w for w in sent.split() if not w in stop_words]\n",
    "        full_str = convert_list_to_string(filtered_sentence)\n",
    "        text2[i] = full_str\n",
    "\n",
    "    #lemmatize with context (adverb, verb)\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    i=-1\n",
    "    for sent in text2:\n",
    "        i+=1\n",
    "        token = sent.split()\n",
    "        j=-1\n",
    "        for word in token:\n",
    "            j+=1\n",
    "            if(word.endswith('ly')):\n",
    "                token[j] = word[:-2]\n",
    "            elif(word.endswith('ing')):\n",
    "                token[j] = word[:-3]\n",
    "\n",
    "        lemmatized_word = [wordnet_lemmatizer.lemmatize(word) for word in token]#first time to remove all the plurals\n",
    "        lemmatized_word2 = [wordnet_lemmatizer.lemmatize(word, 'v') for word in lemmatized_word]#second time to transform all the verbs into their root\n",
    "        full_str = convert_list_to_string(lemmatized_word2)\n",
    "        text2[i] = full_str\n",
    "    \n",
    "    return text2[0].split()\n",
    "\n",
    "text = df.text\n",
    "#text2 = preprocess2(text)\n",
    "\n",
    "text2 = []\n",
    "for i in range(len(text)):\n",
    "    a = pd.Series(text[i])\n",
    "    b = preprocess2(a[0])\n",
    "    text2.append(b)\n",
    "\n",
    "tagged_data = [TaggedDocument(d, [i]) for i, d in enumerate(text2)]\n",
    "tagged_data\n",
    "\n",
    "## Train doc2vec model\n",
    "model = Doc2Vec(tagged_data, vector_size=20, window=2, min_count=1, workers=4, epochs = 20)\n",
    "\n",
    "pickle.dump(model, open('model.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
